% This file is part of the "From Hogg for Gordon" project.
% Copyright 2013 the author (guess who that is).

\documentclass[12pt]{article}
\usepackage{graphics}

\newcommand{\documentname}{\textsl{Note}}
\newcommand{\given}{\,|\,}

\begin{document}

Imagine that you have a model $H$ with a $D$-dimensional parameter vector $\theta$,
and a $N$-dimensional data vector $X$.
By definition---Hogg's definition---if it is permissible to call your model $H$
a ``model'', then you also have a specification of the likelihood function,
$p(X\given\theta, H)$.
Now imagine that it is so extremely expensive to compute the likelihood function that
you have had to pre-compute parts of it on a (possibly highly irregular)
grid of $M$ positions $\theta_m$ in the $D$-dimensional space.
This pre-computation forces you to do all your subsequent analysis
\emph{on this grid}.
That is, in a certain sense your parameter space has just become discrete,
because you can't afford the continuum.

In reality the $\theta$-space is continuous, and your prior pdf $p(\theta\given H)$
is continuous and has support everywhere in the $\theta$-space.
You have two options:
Either you can treat the space as truly discrete, and somehow discretize your prior,
or else you can treat the space as truly continuous, and somehow ``continuum-ize'' your likelihood grid.
In this \documentname, we will consider the latter; we answer the following question:
How can you do continuous parameter estimation and nuisance-parameter marginalization
when the likelihood function is only computed on a finite, discrete grid?

The first thing we will need is some kind of proximity function
$m(\theta)$ that, for any point $\theta$ in the parameter space,
returns the index $m$ that is the identity of the closest (or most
relevant) discrete parameter-space grid point $\theta_m$ to the point
$\theta$.  This function $m(\theta)$ will in general be some kind of
set of cuts in the space, either a point-based tesselation or a set of
hard-set boundaries.  Once we have this function, the
continuum-ization is just the extension of the discrete likelihood
over the domains defined by $m(\theta)$:
\begin{eqnarray}
p(X\given\theta, H)
 &\approx&
p(X\given\theta_{m(\theta)}, H)
\quad ,
\end{eqnarray}
That is, in this approximation the full likelihood function
$p(X\given\theta, H)$ is approximated by a piecewise-constant function
that is constant within each of the $M$ domains, and the constant
value in each domain $m$ is the value at the grid point $\theta_m$.
Recall that the grid can be highly irregular; nothing here requires
regularity or axis alignment or any other nice property for this
(perhaps ill-named) ``grid''.

For inference or marginalization, this approximate likelihood can be
multiplied by any prior $p(\theta\given H)$ in the $\theta$-space, to
make an approximate posterior pdf.  Because the prior won't in general
be constant in domains of $\theta$, the posterior approximation won't
in general be piecewise constant any more.  This is all illustrated
for an ersatz two-dimensional $\theta$ space in
\figurename~\ref{fig:posterior}.  Margialization to a subspace
proceeds as per usual; marginalization is illustrated in
\figurename~\ref{fig:marginal}.

\begin{figure}[p]
\includegraphics[width=\textwidth]{voxels.pdf}
\caption{\textsl{(top-left)}~An example true (non-approximate)
  likelihood function in a two-dimensional $\theta$-space (parameter
  space). \textsl{(top-right)}~The approximate likelihood function
  based on computations of the true likelihood at only $M=64$ control
  points $\theta_m$ indicated by crosses.  The approximate likelihood
  function is piecewise constant in regions defined by a proximity
  funcion $m(\theta)$, which in this case is a Voronoi tesselation
  (nearest-neighbor).  \textsl{(bottom-left)}~A non-trivial prior pdf
  in the $\theta$-space.  \textsl{(bottom-right)}~The approximate
  posterior pdf obtained by multiplying the approximate likelihood by
  the prior.  This approximate posterior is no longer piecewise
  constant, it is now only piecewise continuous.\label{fig:posterior}}
\end{figure}

\begin{figure}[p]
\includegraphics[width=\textwidth]{voxels2.pdf}
\caption{\textsl{(top-left)}~The marginalization of the true
  likelihood shown in \figurename~\ref{fig:posterior} down to the
  horizontal axis.  This marginalization was possible because the
  prior shown in \figurename~\ref{fig:posterior} was separable
  (horizontally and vertically).  The zero level is shown with a grey
  line.  \textsl{(top-right)}~The same but for the marginalized
  approximate likelihood.  The true marginalized likelihood is shown
  again as a smooth grey curve.  \textsl{(bottom-left)}~The same but
  for the marginalized posterior pdf; this differs from the
  marginalized likelihood by a multiplication of the prior in the
  horizontal variable.  \textsl{(bottom-right)}~The same but for the
  marginalized approximate posterior pdf.\label{fig:marginal}}
\end{figure}

The approximate posterior can be sampled directly using MCMC, where
the prior calls are unchanged while likelihood calls take the
parameter position $\theta$ through the proximity function $m(\theta)$
before evaluating (or looking up) the relevant likelihood at the
nearest control point $\theta_m$.  Sampling can also proceed by using
the approximate likelihood to \emph{importance sample} down a sampling
of the prior PDF.  If the prior can be easily and densely sampled, and
the proximity function is written sensibly so it can run on the full
prior sampling quickly, this can be efficient.

\end{document}
