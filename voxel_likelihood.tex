% This file is part of the "From Hogg for Gordon" project.
% Copyright 2013 the author (guess who that is).

\documentclass[12pt]{article}

\newcommand{\documentname}{\textsl{Note}}
\newcommand{\given}{\,|\,}

\begin{document}

Imagine that you have a model $H$ with a $D$-dimensional parameter vector $\theta$,
and a $N$-dimensional data vector $X$.
By definition---Hogg's definition---if it is permissible to call your model $H$
a ``model'', then you also have a specification of the likelihood function,
$p(X\given\theta, H)$.
Now imagine that it so extremely expensive to compute the likelihood function that
you have had to pre-compute parts of it on a grid of $M$ positions $\theta_m$ in
the $D$-dimensional space.
This pre-computation forces you to do all your subsequent analysis
\emph{on this grid}.
That is, in a certain sense your parameter space has just become discrete,
because you can't afford the continuum.

In reality the $\theta$-space is continuous, and your prior pdf $p(\theta\given H)$
is continuous and has support everywhere in the $\theta$-space.
You have two options:
Either you can treat the space as truly discrete, and somehow discretize your prior,
or else you can treat the space as truly continuous, and somehow ``continuum-ize'' your likelihood grid.
In this \documentname, we will consider the latter; we answer the following question:
How can you do continuous parameter estimation and nuisance-parameter marginalization
when the likelihood function is only computed on a finite, discrete grid?

The first thing we will need is some kind of proximity function
$m(\theta)$ that, for any point $\theta$ in the parameter space,
returns the index $m$ that is the identity of the closest (or most
relevant) discrete parameter-space grid point $\theta_m$ to the point
$\theta$.  This function $m(\theta)$ will in general be some kind of
set of cuts in the space, either a point-based tesselation or a set of
hard-set boundaries.  Once we have this function, the
continuum-ization is just the extension of the discrete likelihood
over the domains defined by $m(\theta)$:
\begin{eqnarray}
p(X\given\theta, H)
 &\approx&
p(X\given\theta_{m(\theta)}, H)
\quad ,
\end{eqnarray}
That is, in this approximation the full likelihood function
$p(X\given\theta, H)$ is approximated by a piecewise-constant function
that is constant within each of the $M$ domains $m$, and the constant
value in each domain is the value at the grid point $\theta_m$.

\end{document}
